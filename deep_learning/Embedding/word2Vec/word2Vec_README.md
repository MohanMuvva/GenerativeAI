
# Word2Vec Implementation

## Script Functionality
This notebook explores Word2Vec, an algorithm for generating word embeddings. It includes:
1. **Text Preprocessing**: Tokenizing and cleaning text.
2. **Word Embedding Generation**: Using Word2Vec to represent words as dense vectors.
3. **Visualization**: Exploring word relationships using dimensionality reduction techniques.

## Requirements
The following libraries are required:
- `gensim`
- `matplotlib`
- `sklearn`

Install these with:
```bash
pip install gensim matplotlib scikit-learn
```

## Usage
1. Open the notebook:
   ```bash
   jupyter notebook word2Vec.ipynb
   ```
2. Follow the steps to:
   - Train Word2Vec on a sample corpus.
   - Visualize word embeddings using t-SNE or PCA.

## Outputs:
- Word embeddings.
- Visualizations of word relationships.

## Concluding Note:
This notebook provides a foundation for understanding and applying Word2Vec for word representation tasks. Try it on larger datasets to observe meaningful semantic relationships.
